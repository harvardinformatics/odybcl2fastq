'''
snakemake workflow for 10x single cell

Created on  2019-03-25

@author: Meghan Correa <mportermahoney@g.harvard.edu>
@copyright: 2019 The Presidents and Fellows of Harvard College. All rights reserved.
@license: GPL v2.0
'''

configfile: "/app/odybcl2fastq/snakemake_config.json"
localrules: all, update_lims_db, cp_source_to_output, checksum, publish, demultiplex_10x_cmd, count_10x_cmd, fastqc_cmd, fastq_email, insert_run_into_bauer_db
from odybcl2fastq.run import update_lims_db, setup_run_logger
from odybcl2fastq.parsers.samplesheet import SampleSheet
from odybcl2fastq.emailbuilder.emailbuilder import buildmessage
from odybcl2fastq import config as ody_config
from odybcl2fastq.bauer_db import BauerDB
import odybcl2fastq.util as util
import pandas as pd
import os

# parse sample sheet for sample names
sample_sheet_path = "%s%s/SampleSheet.csv" % (config['source'], config['run'])
with open(sample_sheet_path, 'r') as ln:
    idx = next(i for i, j in enumerate(ln) if j.startswith('[Data]'))
data = pd.read_csv(sample_sheet_path, skiprows=idx+1)

samples = list(data['Sample_ID'])
projects = list(data['Sample_Project'])
STATUS_DIR = 'status'

# set up bauer db for step updates
bauer = BauerDB(sample_sheet_path)

onstart:
    """
    touch processed file to prevent reprocessing
    prepare log, script and status dirs
    """
    shell("mkdir -p {source}{run}/{status}", source={config['source']}, run=config['run'], status=STATUS_DIR)
    shell("touch {source}{run}/{status}/ody.processed", source=config['source'], run=config['run'], status=STATUS_DIR)
    shell("mkdir -p {output}{run}", output={config['output']}, run=config['run'])
    shell("mkdir -p {output}{run}/log", output={config['output']}, run=config['run'])
    shell("mkdir -p {output}{run}/script", output={config['output']}, run=config['run'])
    update_analysis({'status': 'processing'})

rule all:
    """
    final output of workflow
    """
    input:
        expand("{source}{run}/{status}/ody.complete", source=config['source'], run=config['run'], status=STATUS_DIR)

rule insert_run_into_bauer_db:
    """
    insert the run into the bauer_db
    """
    input:
        sample_sheet=expand("{source}{{run}}/SampleSheet.csv", source=config['source'])
    output:
        expand("{source}{{run}}/{status}/analysis_id", source=config['source'], status=STATUS_DIR)
    run:
        bauer = BauerDB(input.sample_sheet[0])
        bauer.insert_run()
        analysis_id = bauer.send_data('requests', {"run": wildcards.run, "status":"processing", "step":"demultiplex"})
        analysis_file_path = '%s%s/%s/analysis_id' % (config['source'], wildcards.run, STATUS_DIR)
        with open(analysis_file_path, 'w+') as f:
            f.write(str(analysis_id))

rule demultiplex_10x_cmd:
    """
    build a bash file with the demux cmd
    """
    input:
        expand("{source}{{run}}/{status}/analysis_id", source=config['source'], status=STATUS_DIR),
        run_dir=expand("{source}{{run}}/SampleSheet.csv", source=config['source'])
    output:
        expand("{output}{{run}}/script/demultiplex_10x.sh", output=config['output'])
    shell:
        """
        dual_index="--ignore-dual-index"
        if [ ! -z "{config[atac]}" ]; then
            dual_index=""
        fi
        cmd="#!/bin/bash\n"
        cmd+="ulimit -u \$(ulimit -Hu)\n"
        cmd+="exit_code=0\n"
        cmd+="mkdir -p {config[output_slurm]}{wildcards.run}/fastq\n"
        cmd+="mkdir -p /scratch/{wildcards.run}_fastq_\$SLURM_JOB_ID\n"
        cmd+="cd /scratch/{wildcards.run}_fastq_\$SLURM_JOB_ID\n"
        cmd+="cellranger{config[atac]} mkfastq $dual_index --run={config[source_slurm]}{wildcards.run} --samplesheet={config[source_slurm]}{wildcards.run}/SampleSheet.csv --output-dir=/scratch/{wildcards.run}_fastq_\$SLURM_JOB_ID --localmem=\$((SLURM_MEM_PER_NODE/1000)) --localcores=\$SLURM_JOB_CPUS_PER_NODE || exit_code=\$?\n"
        cmd+="rsync -rtl --safe-links --perms --chmod=Dug=rwx,Fug=rw /scratch/{wildcards.run}_fastq_\$SLURM_JOB_ID/ {config[output_slurm]}{wildcards.run}/fastq/ || exit_code=\$((exit_code + \$?))\n"
        cmd+="rm -rf /scratch/{wildcards.run}_fastq_\$SLURM_JOB_ID\n"
        cmd+="exit \$exit_code"
        echo "$cmd" >> {output}
        chmod 777 {output}
        """

rule demultiplex_10x:
    """
    run bash file for demux
    the slurm_submit.py script will add slurm params to the top of this file
    """
    input:
        expand("{output}{{run}}/script/demultiplex_10x.sh", output=config['output'])
    output:
        touch(expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR))
    run:
        update_analysis({'step': 'demultiplex', 'status': 'processing'})
        shell("{input}")

rule count_10x_cmd:
    """
    build a bash file with the demux cmd
    """
    input:
        expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR),
        expand("{source}{{run}}/{status}/fastq_email.processed", source=config['source'], status=STATUS_DIR)
    output:
        expand("{output}{{run}}/script/{{project}}.{{sample}}_count.sh", output=config['output'])
    shell:
        """
        fastq_path="{config[output_slurm]}{wildcards.run}/fastq/{wildcards.project}/{wildcards.sample}"
        transcriptome="--transcriptome={config[ref]}"
        if [ ! -z "{config[atac]}" ]; then
            transcriptome="--reference={config[ref]}"
        fi
        cmd="#!/bin/bash\n"
        cmd+="ulimit -u \$(ulimit -Hu)\n"
        cmd+="exit_code=0\n"
        cmd+="mkdir -p {config[output_slurm]}{wildcards.run}/count\n"
        cmd+="mkdir -p /scratch/{wildcards.run}_{wildcards.sample}_\$SLURM_JOB_ID\n"
        cmd+="cd /scratch/{wildcards.run}_{wildcards.sample}_\$SLURM_JOB_ID\n"
        cmd+="cellranger{config[atac]} count --id={wildcards.sample} $transcriptome --sample={wildcards.sample} --fastqs=$fastq_path --localmem=\$((SLURM_MEM_PER_NODE/1000)) --localcores=\$SLURM_JOB_CPUS_PER_NODE || exit_code=\$?\n\n"
        cmd+="rsync -rtl --safe-links --perms --chmod=Dug=rwx,Fug=rw /scratch/{wildcards.run}_{wildcards.sample}_\$SLURM_JOB_ID/{wildcards.sample}/ {config[output_slurm]}{wildcards.run}/count/{wildcards.sample}/ || exit_code=\$((exit_code + \$?))\n"
        cmd+="rm -rf /scratch/{wildcards.run}_{wildcards.sample}_\$SLURM_JOB_ID\n"
        cmd+="exit \$exit_code"
        echo "$cmd" >> {output}
        chmod 777 {output}
        """

rule count_10x:
    """
    run bash file for count
    the slurm_submit.py script will add slurm params to the top of this file
    """
    input:
        script=expand("{output}{{run}}/script/{{project}}.{{sample}}_count.sh", output=config['output'])
    output:
        touch(expand("{source}{{run}}/{status}/{{project}}.{{sample}}_count.processed", source=config['source'], status=STATUS_DIR))
    shell:
        """
        {input}
        """

rule update_lims_db:
    """
    connect the submission with the run in lims
    """
    input:
        expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR),
        sample_sheet=expand("{source}{{run}}/SampleSheet.csv", source=config['source'])
    output:
        touch(expand("{source}{{run}}/{status}/update_lims_db.processed", source=config['source'], status=STATUS_DIR))
    run:
        setup_run_logger(wildcards.run, False)
        sample_sheet = SampleSheet(input.sample_sheet[0])
        instrument = sample_sheet.get_instrument()
        run_folder = config['source'] + wildcards.run
        update_lims_db(run_folder, sample_sheet.sections, instrument)
        # also update step to fastqc
        update_analysis({'step': 'quality', 'status': 'processing'})

rule fastqc_cmd:
    """
    build a bash file with the fastqc cmd
    """
    input:
        ancient(expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR))
    output:
        expand("{output}{{run}}/script/fastqc.sh", output=config['output'])
    shell:
        """
        cmd="#!/bin/bash\n"
        cmd+="ulimit -u \$(ulimit -Hu)\n"
        cmd+="mkdir -p {config[output_slurm]}{wildcards.run}/QC\n"
        cmd+="fastqc -o {config[output_slurm]}{wildcards.run}/QC --threads 1 $(find {config[output_slurm]}{wildcards.run}/fastq/ -name *.fastq.gz -not -name Undetermined* -print0 | xargs -0)"
        echo "$cmd" >> {output}
        chmod 777 {output}
        """

rule fastqc:
    """
    run bash file for fastqc
    the slurm_submit.py script will add slurm params to the top of this file
    """
    input:
        expand("{output}{{run}}/script/fastqc.sh", output=config['output'])
    output:
        touch(expand("{source}{{run}}/{status}/fastqc.processed", source=config['source'], status=STATUS_DIR))
    shell:
        """
        {input}
        """

rule cp_source_to_output:
    """
    copy a few files from source to output dir
    """
    input:
        expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR)
    params:
        sample_sheet="SampleSheet.csv",
        run_info="RunInfo.xml",
        interop="InterOp",
        nextseq_run_params="RunParameters.xml",
        hiseq_run_params="runParameters.xml"
    output:
        sample_sheet=expand("{output}{{run}}/SampleSheet.csv", output=config['output']),
        run_info=expand("{output}{{run}}/RunInfo.xml", output=config['output'])
    shell:
        """
        cp {config[source]}{wildcards.run}/{params.sample_sheet} {output.sample_sheet}
        cp {config[source]}{wildcards.run}/{params.run_info} {output.run_info}
        rsync -rtl --safe-links --perms --chmod=Dug=rwx,Fug=rw {config[source]}{wildcards.run}/{params.interop}/ {config[output]}{wildcards.run}/InterOp/
        # copy these if they exist
        cp {config[source]}{wildcards.run}/{params.nextseq_run_params} {config[output]}{wildcards.run}/{params.nextseq_run_params} 2>/dev/null || :
        cp {config[source]}{wildcards.run}/{params.hiseq_run_params} {config[output]}{wildcards.run}/{params.hiseq_run_params} 2>/dev/null || :

        """

rule checksum:
    """
    calculate checksum for all the fastq files
    """
    input:
        expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR)
    output:
        checksum=expand("{output}{{run}}/md5sum.txt", output=config['output']),
    shell:
        """
        files=$(find {config[output]}{wildcards.run}/ -name *.fastq.gz -print0 | xargs -0)
        md5sum $files > {output.checksum}
        """

rule fastq_email:
    """
    copy fastq files to final and send an email that fastq files are ready, this step is run only for jobs
    running count
    """
    input:
        checksum=expand("{output}{{run}}/md5sum.txt", output=config['output']),
        fastqc=expand("{source}{{run}}/{status}/fastqc.processed", source=config['source'], status=STATUS_DIR),
        lims=expand("{source}{{run}}/{status}/update_lims_db.processed", source=config['source'], status=STATUS_DIR),
        demultiplex=expand("{source}{{run}}/{status}/demultiplex.processed", source=config['source'], status=STATUS_DIR),
        sample_sheet=expand("{output}{{run}}/SampleSheet.csv", output=config['output']),
        run_info=expand("{output}{{run}}/RunInfo.xml", output=config['output'])
    output:
        touch(expand("{source}{{run}}/{status}/fastq_email.processed", source=config['source'], status=STATUS_DIR))
    run:
        update_analysis({'step': 'count', 'status': 'processing'})
        shell("cp -r {config[output]}{wildcards.run} {config[published]}")
        subject = 'Demultiplex Summary for: %s (count pending)' % config['run']
        send_success_email(subject)

def publish_input(wildcards):
    """
    determine which files need to be ready to publish the run
    count is not run if there is not reference genome
    """
    input = {
        'checksum': "%s%s/md5sum.txt" % (config['output'], wildcards.run),
        'fastqc': "%s%s/%s/fastqc.processed" % (config['source'], wildcards.run, STATUS_DIR),
        'lims': "%s%s/%s/update_lims_db.processed" % (config['source'], wildcards.run, STATUS_DIR),
        'sample_sheet': "%s%s/SampleSheet.csv" % (config['output'], wildcards.run),
        'run_info': "%s%s/RunInfo.xml" % (config['output'], wildcards.run)
    }
    # if reference is provided then run count
    if config['ref']:
        # copy fastq to final and send an email that count is pending
        input['email'] = "%s%s/%s/fastq_email.processed" % (config['source'], wildcards.run, STATUS_DIR)
        for i, sample in enumerate(samples):
            project = projects[i]
            key = 'count_%s.%s' % (project, sample)
            input[key] = "%s%s/%s/%s.%s_count.processed" % (config['source'], wildcards.run, STATUS_DIR, project, sample)
    return input


rule publish:
    """
    copy all output to a published location
    """
    input:
        unpack(publish_input)
    output:
        touch(expand("{source}{{run}}/{status}/ody.complete", source=config['source'], status=STATUS_DIR))
    run:
        update_analysis({'step': 'publish', 'status': 'processing'})
        shell("rsync -rtl --perms --chmod=Dug=rwx,Do=rx,Fug=rw,Fo=r {config[output]}{wildcards.run}/ {config[published]}{wildcards.run}/")
        subject = 'Demultiplex Summary for: %s' % config['run']
        send_success_email(subject)

onsuccess:
    update_analysis({'status': 'complete'})

onerror:
    message = 'run %s failed\n see logs here: %s%s.log\n' % (config['run'], config['log'], config['run'])
    subject = 'Run Failed: %s' % config['run']
    sent = buildmessage(message, subject, {}, ody_config.EMAIL['from_email'], ody_config.EMAIL['admin_email'])
    update_analysis({'status': 'failed'})

def get_summary_data(cmd, run, ss_file):
    sample_sheet = util.get_file_contents(ss_file)
    assumptions = [
        'chromium single-cell RNA-seq',
        'ignoring the second index on a dual-indexed flowcell'
    ]
    versions = [
        'cellranger 3.1.0',
        'fastqc 0.11.8'
    ]
    if config['ref']:
        assumptions.append('reference genome %s, see annotation under versions below' % os.path.basename(config['ref']))
        assumptions.append('samples are full cell, not nuclei')
        versions.append('annotation gtf: %s' % config['gtf'])
    summary_data = {
        'fastq_url': ody_config.FASTQ_URL,
        'fastq_dir': ody_config.MOUNT_DIR,
        'versions': versions,
        'run': run,
        'run_folder': run,
        'cmd': cmd,
        'sample_sheet_file': ss_file,
        'sample_sheet': sample_sheet,
        'assumptions': assumptions
    }
    return summary_data

def send_success_email(subject):
    message = 'run %s completed successfully\n see logs here: %s%s.log\n' % (config['run'], config['log'], config['run'])
    cmd_file = '%s%s/script/demultiplex_10x.sh' % (config['output'], config['run'])
    cmd = util.get_file_contents(cmd_file)
    ss_file = '%s%s/SampleSheet.csv' % (config['source'], config['run'])
    summary_data = get_summary_data(cmd, config['run'], ss_file)
    sent = buildmessage(message, subject, summary_data, ody_config.EMAIL['from_email'], ody_config.EMAIL['to_email'], '10x_summary.html')

def update_analysis(data):
    analysis_file_path = '%s%s/%s/analysis_id' % (config['source'], config['run'], STATUS_DIR)
    if os.path.isfile(analysis_file_path):
        with open(analysis_file_path, 'r') as ln:
            analysis_id = ln.readline().strip()
        bauer.update_data('requests', analysis_id, data)
